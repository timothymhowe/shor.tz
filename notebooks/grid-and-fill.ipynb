{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, I detail the method by which a grid is generated and subsequently filled."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "WEEKDAY_GRID_SIZE = 15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  if a square is empty, 0.  If a square is black, 1.\n",
    "def bin_gridify(raw_grids):\n",
    "    binary_grids = []\n",
    "    for grid in raw_grids:\n",
    "        new_grid = [1 if x == \".\" else 0 for x in grid ]\n",
    "        binary_grids = binary_grids + [new_grid]\n",
    "    return binary_grids\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dow_dict = pickle.load(open('../bin/pickles/dow_dict.pickle','rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_set = []\n",
    "for day in dow_dict.keys():\n",
    "    grid_list = dow_dict[day]\n",
    "    grid_list = bin_gridify(grid_list)\n",
    "    data_set = data_set + [grid_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mon = data_set[0]\n",
    "mon = [np.array(grid).reshape(15,15) for grid in mon if len(grid) == (15*15)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# defining a function that checks the symmetry of the\n",
    "def check_symmetry(dim,grid):\n",
    "    leng = dim*dim\n",
    "    half = (leng-1)//2\n",
    "    array = grid.reshape(leng,)\n",
    "    front = array[:half]\n",
    "    back = array[-half:]\n",
    "    return (front == np.flip(back)).all()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "check_symmetry(15,mon[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    plt.subplot(5,5,1+i)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(mon[i],cmap=\"Greys\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creating a gan, we start by defining a standalone discriminator\n",
    "from keras import Sequential\n",
    "from keras.layers import LeakyReLU, Conv2D, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def define_discriminator(in_shape=(15,15,1)):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), strides=(2,2),padding='same',input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0,4))\n",
    "\n",
    "    model.add(Conv2D(64,(3,3),strides=(2,2),padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "    opt = Adam(lr=0.0002,beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# train the discriminator model\n",
    "def train_discriminator(model, dataset, n_iter=100, n_batch=256):\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_iter):\n",
    "\t\t# get randomly selected 'real' samples\n",
    "\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\t# update discriminator on real samples\n",
    "\t\t_, real_acc = model.train_on_batch(X_real, y_real)\n",
    "\t\t# generate 'fake' examples\n",
    "\t\tX_fake, y_fake = generate_fake_samples(half_batch)\n",
    "\t\t# update discriminator on fake samples\n",
    "\t\t_, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
    "\t\t# summarize performance\n",
    "\t\tprint('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))\n",
    "\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# choose random instances\n",
    "\tix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX = dataset[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = np.ones((n_samples,1))\n",
    "\treturn X, y\n",
    "\n",
    "\n",
    "# generate n fake samples with class labels\n",
    "def generate_fake_samples(n_samples):\n",
    "\t# generate uniform random numbers in [0,1]\n",
    "\tX = np.random.binomial(.5,.5,15*15*n_samples)\n",
    "\t# reshape into a batch of grayscale images\n",
    "\tX = X.reshape((n_samples, 15, 15, 1))\n",
    "\t# generate 'fake' class labels (0)\n",
    "\ty = np.zeros((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "model = define_discriminator()\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,to_file='../exports/discriminator.png', show_shapes=True, show_layer_names=True);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_discriminator(model, dataset, n_iter=100, n_batch=256):\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_iter):\n",
    "\t\t# get randomly selected 'real' samples\n",
    "\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\t# update discriminator on real samples\n",
    "\t\t_, real_acc = model.train_on_batch(X_real, y_real)\n",
    "\t\t# generate 'fake' examples\n",
    "\t\tX_fake, y_fake = generate_fake_samples(half_batch)\n",
    "\t\t# update discriminator on fake samples\n",
    "\t\t_, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
    "\t\t# summarize performance\n",
    "\t\tprint('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))\n",
    "\n",
    "# define the discriminator model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "X = np.expand_dims(mon,axis=-1)\n",
    "X = X.astype('float32')\n",
    "\n",
    "model = define_discriminator()\n",
    "# load image data\n",
    "dataset = X\n",
    "# fit the model\n",
    "train_discriminator(model, dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_fake_samples(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "# creating the generator model\n",
    "def define_generator(dim):\n",
    "    model = Sequential()\n",
    "\n",
    "    # creating nodes for a 15 * 15 grid\n",
    "    n_nodes = 128 * 5 * 5\n",
    "    model.add(Dense(n_nodes,input_dim=dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((5,5,128)))\n",
    "\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(1,1), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2DTranspose(128,(3,3),strides=(3,3),padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1,(7,7), activation='sigmoid',padding='same'))\n",
    "    return model\n",
    "\n",
    "dim = 500\n",
    "\n",
    "model = define_generator(dim)\n",
    "model.summary()\n",
    "\n",
    "plot_model(model,to_file='../exports/generator.png',show_shapes=True,show_layer_names=True)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "from numpy.random import randn\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "def generate_fake_samples(g_model,latent_dim,n_samples):\n",
    "    # generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = g_model.predict(x_input)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = np.zeros((n_samples, 1))\n",
    "\treturn X, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X, _ = generate_fake_samples(model, dim, 25)\n",
    "# # plot the generated samples\n",
    "# for i in range(25):\n",
    "# \t# define subplot\n",
    "# \tplt.subplot(5, 5, 1 + i)\n",
    "# \t# turn off axis labels\n",
    "# \tplt.axis('off')\n",
    "# \t# plot single image\n",
    "# \tplt.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "# # show the figure\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gan_builder(disc_model,gen_model):\n",
    "    disc_model.trainable=True\n",
    "    model = Sequential()\n",
    "    model.add(gen_model)\n",
    "    model.add(disc_model)\n",
    "    optimizer = Adam(lr=0.0002,beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# size of the latent space\n",
    "dim = 100\n",
    "# create the discriminator\n",
    "d_model = define_discriminator()\n",
    "# create the generator\n",
    "g_model = define_generator(dim)\n",
    "# create the gan\n",
    "gan_model = gan_builder(g_model, d_model)\n",
    "# summarize gan model\n",
    "gan_model.summary()\n",
    "# plot gan model\n",
    "plot_model(gan_model, to_file='gan_plot.png', show_shapes=True, show_layer_names=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_gan(disc,gen,data,dim,n_epochs=100,n_batch=256):\n",
    "    #batches per epoch\n",
    "    bpe = data.shape // n_batch\n",
    "    bpe = int(bpe)\n",
    "    # size of half batch\n",
    "    half_b = n_batch//2\n",
    "    half_b = int(half_b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cudnn-1",
   "language": "python",
   "display_name": "Tensorflow (GPU)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}